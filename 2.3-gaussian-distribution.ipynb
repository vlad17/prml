{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "from IPython.display import display, Math, Latex\n",
      "import scipy.stats\n",
      "maths = lambda s: display(Math(s))\n",
      "latex = lambda s: display(Latex(s))\n",
      "normalise = lambda l: l / sum(l)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Gaussian or normal distribution is a commonly used continuous distribution over a single real random variable (rv) that can be extended naturally to random vectors. For instance, as Bishop notes, the Central Limit Theorem states that the average of $N$ iid rvs converges in distribution to a normal assuming finite mean and variance.\n",
      "\n",
      "For an alternative viewpoint, let's try to derive the Gaussian from the maximization of differential entropy $H[p]=-\\int p(\\textbf{x})\\log p(\\textbf{x})d\\textbf{x}$ to derive the distribution \"naturally\" (ex. 2.14):\n",
      "\n",
      "$$\n",
      "\\max_p H[p]\\, \\text{s.t.}\\, \\int p =1;\\, \\mathbb{E}(\\textbf{x})=\\boldsymbol{\\mu};\\,  \\mathrm{cov}(\\textbf{x})=\\boldsymbol{\\Sigma}\n",
      "$$\n",
      "\n",
      "This is a vectorized optimization of form presented in Section 1.6, pg. 52-54 of PRML. Note the above does not contain the implicit additional constraint that $p\\ge 0$ and the implied constraint that $p\\in L^1$. Here and below, $L^1$ denotes the Banach space over functions $f\\in\\{\\mathbb{R}^n\\rightarrow\\mathbb{R}\\}$ with a finite norm $\\|f\\|=\\int_{\\mathbb{R}^n}\\left|f\\right|$. This proof will rely on some properties of $L^1$ taken for granted.\n",
      "\n",
      "Suppose $\\textbf{x}\\in\\mathbb{R}^n$ and let $\\textbf{e}_i$ be the $i$-th standard basis vector. We denote the Lagrangian:\n",
      "\n",
      "$$\n",
      "L[p] = H[p]+\\lambda\\left(\\|p\\| -1\\right)+\\boldsymbol{\\lambda}_0\\cdot\\left(\\mathbb{E}(\\textbf{x})-\\boldsymbol{\\mu}\\right)+\\sum_{i=1}^n\\boldsymbol{\\lambda}_i\\cdot(\\mathrm{cov}\\left(\\textbf{x})-\\boldsymbol{\\Sigma}\\right)\\textbf{e}_i\n",
      "$$\n",
      "\n",
      "Now we proceed with unconstrained functional optimization of $L$ wrt $p$. Letting $L[p] = c+\\int M(p(\\textbf{x}), \\textbf{x})d\\textbf{x}$ for some $c$ independent of $p$, by linearity of the integral:\n",
      "\n",
      "$$\n",
      "M(p, \\textbf{x})=-p\\log p+\\lambda p+p\\boldsymbol{\\lambda}_0\\cdot \\textbf{x} +p\\sum_{i=1}^n\\boldsymbol{\\lambda}_i\\cdot (\\textbf{x}-\\boldsymbol{\\mu})(\\textbf{x}-\\boldsymbol{\\mu})^\\top \\textbf{e}_i\n",
      "$$\n",
      "\n",
      "Note that $M$ is a differentiable function with real-valued, non-functional parameters, so for any sufficiently small $\\delta$ where $p\\neq 0$ (since $H[0]=0$ by definition, $M=0$ there too):\n",
      "\n",
      "$$\n",
      "M(a+\\delta,\\textbf{b})=M(a)+\\delta M(a,\\textbf{b})+O(\\delta^2)\n",
      "$$\n",
      "\n",
      "Taking care that the $O(\\delta^2)$ term obscures a dependence on $a$ above, we try varying $p$ in $L$ by a small function $\\epsilon \\eta$:\n",
      "\n",
      "$$\n",
      "L[p+\\epsilon\\eta]=c+\\int M(p(\\textbf{x}+\\epsilon\\eta(\\textbf{x}), \\textbf{x})d\\textbf{x}=\n",
      "c+\\int M(p(\\textbf{x}),\\textbf{x})+\\epsilon\\eta(\\textbf{x})\\partial_1M(p(\\textbf{x}),\\textbf{x})d\\textbf{x}+O(\\epsilon^2)\\\n",
      "$$\n",
      "\n",
      "The above step requires some justification. First, observe the use of the chain rule and disambiguating $\\partial_i$ notation, which represents the partial derivative wrt the $i$-th parameter. Second, the above can hold for all sufficiently small $\\epsilon$ since $\\eta$ must be in $L^1$ (it is closed over addition). We also require the fact the $M$ is $L^1$ itself in order to pull out the error term $O(\\epsilon^2)$ from the integral [TODO: prove $M$ is $L^1$].\n",
      "\n",
      "By selection of $\\eta$ to be zero everywhere but any fixed neigborhood of $\\hat{\\textbf{x}}$, $\\partial_1M=0$ for all $\\hat{\\textbf{x}}$ is necessary for there to be no $\\Omega(\\epsilon)$ term. Expanding the partial derivative we find:\n",
      "\n",
      "$$\n",
      "-\\log p(\\textbf{x})-1+\\lambda+\\boldsymbol{\\lambda}_0\\cdot \\textbf{x}+\\sum_{i=1}^n(\\textbf{x}-\\boldsymbol{\\mu})\\cdot\\boldsymbol{ \\lambda}_i \\textbf{e}_i\\cdot(\\textbf{x}-\\boldsymbol{\\mu})=0\n",
      "$$\n",
      "\n",
      "Note we rely on the symmetry of the dot product above. By linearity of matrix multiplication, the above simplifies to:\n",
      "\n",
      "$$\n",
      "(*) \\log p(\\textbf{x})=-1+\\lambda+\\boldsymbol{\\lambda}_0\\cdot \\textbf{x}+(\\textbf{x}-\\boldsymbol{\\mu})^\\top\\Lambda(\\textbf{x}-\\boldsymbol{\\mu})\n",
      "$$\n",
      "\n",
      "We let $\\Lambda=\\sum_{i=1}^n\\boldsymbol{\\lambda}_i \\textbf{e}_i^\\top$, the matrix of column vectors $\\boldsymbol{\\lambda}_i$.\n",
      "\n",
      "The condition above is a **necessary** condition for an extremal to a functional; that is, any $f$ within an $\\epsilon$-ball of any satisfying $p$, the Lagrangian improves by at most $O(\\epsilon^2)$. To prove that is a maximum, we must show the error term $O(\\epsilon^2)$ is always negative in a small neighborhood. Indeed, expanding (note we integrate only over the support $S$ of $p$, as $\\partial_1^2M=0$ otherwise):\n",
      "\n",
      "$$\n",
      "M(p(\\textbf{x})+\\epsilon\\eta(\\textbf{x}),\\textbf{x})=\\cdots+\\epsilon^2\\eta(\\textbf{x})^2\\partial_1^2M(p(\\textbf{x}),\\textbf{x})+O(\\epsilon^3)=\\cdots-\\epsilon^2\\eta(\\textbf{x})^2p(\\textbf{x})^{-1}+O(\\epsilon^3)\n",
      "$$\n",
      "\n",
      "As promised we integrate out the last two terms:\n",
      "\n",
      "$$\n",
      "\\int_S -\\epsilon^2\\eta(\\textbf{x})^2p(\\textbf{x})^{-1}+O(\\left|\\epsilon\\eta(\\textbf{x})\\right|^3)d\\textbf{x}\n",
      "=-\\epsilon^2\\int_S\\eta^2p^{-1}+O(\\epsilon^3)\n",
      "$$\n",
      "\n",
      "This inequality is justified by observing $\\eta^3$ is $L^1$ because $\\eta$ is [TODO: is this true?]. Then to show the above solution is **sufficient** we may show the above integral is positive. Note that $p$ is positive on $S$, so the only way the above integral can be 0 is if $\\eta(S)=\\{0\\}$ [TODO: can we use an $L^1$ property to say this?].\n",
      "\n",
      "We must have that for any $\\textbf{y}\\in \\overline{S}$, $\\eta(\\textbf{y})\\ge 0$ (else $p+\\epsilon\\eta$ is not a probability density). But then:\n",
      "\n",
      "$$\n",
      "1=\\int p+\\epsilon\\eta=\\int_Sp+\\epsilon\\int_{\\overline{S}}\\eta=1+\\epsilon\\int_{\\overline{S}}\\left|\\eta\\right|\\implies \\|\\eta\\|=0\n",
      "$$\n",
      "\n",
      "So any such continuous $\\eta$ must be 0. In turn, $L[p+\\epsilon\\eta]-L[p]< 0$ for sufficiently small $\\epsilon$ and nonzero $\\eta$ (since the lower-order second term eclipses the higher-order ones). Since there is only one local maximum, and no minima, this must be a global maximum. Suppose for contradiction there is a continuous $f\\in L^1$ for which $L[f]>L[p]$, with $p$ as our local maximum defined by $(*)$. \n",
      "\n",
      "Connectivity and completeness of $L^1$ without the constraints is contradictory... (then tighten back to original problem) [TODO complete this]\n",
      "\n",
      "Returning to the derived necessary and sufficient condition for our maximal distribution, $(*)$ we find that $p$ must be:\n",
      "\n",
      "[TODO gaussian formula]\n",
      "\n",
      "[TODO discuss differential entropy]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}